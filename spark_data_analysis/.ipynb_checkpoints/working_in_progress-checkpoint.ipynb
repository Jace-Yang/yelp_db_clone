{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9acadf7-b171-4f3b-bd78-28eb01805374",
   "metadata": {},
   "source": [
    "# Pyspark tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1678f1a7-f4bb-41f6-965f-9a88e088dc01",
   "metadata": {},
   "source": [
    "## Envionment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20660a8-cb10-4e6c-b854-526625e23b09",
   "metadata": {},
   "source": [
    "**Step 0:** Clone the repository to the cluster's local disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8fc824-b802-4a49-bc34-d89c0e5fb7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Jace-Yang/yelp_db_clone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d8d680-0c2c-41d1-9be4-903e95592f20",
   "metadata": {},
   "source": [
    "**Step 1:** Every time you create a new cluster, download external package in order to parse XML files: spark-xml with version 2.12-0.14.0 to support Spark 3.1.2 and Scala 2.12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70857559-e134-4a9b-86a3-44dcae089040",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo hdfs dfs -get gs://csee4121/homework2/spark-xml_2.12-0.14.0.jar /usr/lib/spark/jars/\n",
    "    # Reference: https://csee-4121-2022.github.io/homeworks/hw2.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85096d5e-d737-4564-954c-a2d17126f714",
   "metadata": {},
   "source": [
    "> Note: if you are using multiple GCP dataproc nodes, run `sudo hdfs dfs -get gs://csee4121/homework2/spark-xml_2.12-0.14.0.jar /usr/lib/spark/jars/` on every worker VM machines by SSH them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c80908-df21-4d32-a369-d28d8bbfd574",
   "metadata": {},
   "source": [
    "**Step 2:** Download dataset from [yelp dataset](https://www.yelp.com/dataset/documentation/main) and upload all json files except the `photos` into a GCP bucket. In this case the bucket name is `coms4111`, and I placed it into a directory that jupyterlab can directly access it through `GCS` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06314524-2185-4a94-a202-396ed6a7bd80",
   "metadata": {},
   "source": [
    "**Step 3:** Move data from GS into a HDFS directory every time you create a new cluster. We do this by moving data into the local disk first, then to HDFS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1454a696-4f01-428a-a72d-81139a4ec724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://coms4111/notebooks/jupyter/data/yelp_academic_dataset_business.json...\n",
      "Copying gs://coms4111/notebooks/jupyter/data/yelp_academic_dataset_checkin.json...\n",
      "Copying gs://coms4111/notebooks/jupyter/data/yelp_academic_dataset_review.json...\n",
      "Copying gs://coms4111/notebooks/jupyter/data/yelp_academic_dataset_tip.json...  \n",
      "/ [4 files][  5.5 GiB/  5.5 GiB]   54.4 MiB/s                                   \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Copying gs://coms4111/notebooks/jupyter/data/yelp_academic_dataset_user.json...\n",
      "/ [5 files][  8.6 GiB/  8.6 GiB]   92.4 MiB/s                                   \n",
      "Operation completed over 5 objects/8.6 GiB.                                      \n"
     ]
    }
   ],
   "source": [
    "# Gs -> Local\n",
    "!mkdir yelp_db_clone/data/\n",
    "!gsutil cp gs://coms4111/notebooks/jupyter/data/*.json file:///yelp_db_clone/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c1c8b21-d60b-488c-a0a9-ec8ca6b33a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local -> HDFS\n",
    "!hdfs dfs -cp -f file:///yelp_db_clone/data/* hdfs:///user/dataproc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "21c91e49-4560-41fb-a7fe-4fc870a5830d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\n",
      "-rw-r--r--   2 root hadoop  118863795 2022-05-03 20:19 hdfs:///user/dataproc/yelp_academic_dataset_business.json\n",
      "-rw-r--r--   2 root hadoop  286958945 2022-05-03 20:19 hdfs:///user/dataproc/yelp_academic_dataset_checkin.json\n",
      "-rw-r--r--   2 root hadoop 5341868833 2022-05-03 20:20 hdfs:///user/dataproc/yelp_academic_dataset_review.json\n",
      "-rw-r--r--   2 root hadoop  180604475 2022-05-03 20:20 hdfs:///user/dataproc/yelp_academic_dataset_tip.json\n",
      "-rw-r--r--   2 root hadoop 3363329011 2022-05-03 20:20 hdfs:///user/dataproc/yelp_academic_dataset_user.json\n"
     ]
    }
   ],
   "source": [
    "# Check whether data is now in HDFS!\n",
    "!hdfs dfs -ls hdfs:///user/dataproc/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb6c1b2-93c4-4929-8c23-bdece75516e0",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c873dcf-a803-445f-80a3-5ff96e9fd958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dd888f2-4e88-4318-909d-c11f19e30953",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/03 20:44:01 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "22/05/03 20:44:01 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "22/05/03 20:44:01 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/05/03 20:44:01 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd4ad24-1bb3-4122-9385-71e90ee403a1",
   "metadata": {},
   "source": [
    "### Read Data & Print Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "622cc42f-af2e-4ab9-b354-63d75eb36479",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:=======================================================> (39 + 1) / 40]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- cool: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "review = spark.read.json('hdfs:///user/dataproc/yelp_academic_dataset_review.json')\n",
    "review.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13859699-634a-45d9-bbdb-395ee9c74605",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- address: string (nullable = true)\n",
      " |-- attributes: struct (nullable = true)\n",
      " |    |-- AcceptsInsurance: string (nullable = true)\n",
      " |    |-- AgesAllowed: string (nullable = true)\n",
      " |    |-- Alcohol: string (nullable = true)\n",
      " |    |-- Ambience: string (nullable = true)\n",
      " |    |-- BYOB: string (nullable = true)\n",
      " |    |-- BYOBCorkage: string (nullable = true)\n",
      " |    |-- BestNights: string (nullable = true)\n",
      " |    |-- BikeParking: string (nullable = true)\n",
      " |    |-- BusinessAcceptsBitcoin: string (nullable = true)\n",
      " |    |-- BusinessAcceptsCreditCards: string (nullable = true)\n",
      " |    |-- BusinessParking: string (nullable = true)\n",
      " |    |-- ByAppointmentOnly: string (nullable = true)\n",
      " |    |-- Caters: string (nullable = true)\n",
      " |    |-- CoatCheck: string (nullable = true)\n",
      " |    |-- Corkage: string (nullable = true)\n",
      " |    |-- DietaryRestrictions: string (nullable = true)\n",
      " |    |-- DogsAllowed: string (nullable = true)\n",
      " |    |-- DriveThru: string (nullable = true)\n",
      " |    |-- GoodForDancing: string (nullable = true)\n",
      " |    |-- GoodForKids: string (nullable = true)\n",
      " |    |-- GoodForMeal: string (nullable = true)\n",
      " |    |-- HairSpecializesIn: string (nullable = true)\n",
      " |    |-- HappyHour: string (nullable = true)\n",
      " |    |-- HasTV: string (nullable = true)\n",
      " |    |-- Music: string (nullable = true)\n",
      " |    |-- NoiseLevel: string (nullable = true)\n",
      " |    |-- Open24Hours: string (nullable = true)\n",
      " |    |-- OutdoorSeating: string (nullable = true)\n",
      " |    |-- RestaurantsAttire: string (nullable = true)\n",
      " |    |-- RestaurantsCounterService: string (nullable = true)\n",
      " |    |-- RestaurantsDelivery: string (nullable = true)\n",
      " |    |-- RestaurantsGoodForGroups: string (nullable = true)\n",
      " |    |-- RestaurantsPriceRange2: string (nullable = true)\n",
      " |    |-- RestaurantsReservations: string (nullable = true)\n",
      " |    |-- RestaurantsTableService: string (nullable = true)\n",
      " |    |-- RestaurantsTakeOut: string (nullable = true)\n",
      " |    |-- Smoking: string (nullable = true)\n",
      " |    |-- WheelchairAccessible: string (nullable = true)\n",
      " |    |-- WiFi: string (nullable = true)\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- hours: struct (nullable = true)\n",
      " |    |-- Friday: string (nullable = true)\n",
      " |    |-- Monday: string (nullable = true)\n",
      " |    |-- Saturday: string (nullable = true)\n",
      " |    |-- Sunday: string (nullable = true)\n",
      " |    |-- Thursday: string (nullable = true)\n",
      " |    |-- Tuesday: string (nullable = true)\n",
      " |    |-- Wednesday: string (nullable = true)\n",
      " |-- is_open: long (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- postal_code: string (nullable = true)\n",
      " |-- review_count: long (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/03 20:44:25 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "business = spark.read.json('hdfs:///user/dataproc/yelp_academic_dataset_business.json')\n",
    "business.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6190e1da-cbb6-4614-bd45-c8c1dc5fcbee",
   "metadata": {},
   "source": [
    "- Here we see that spark allows semi-structure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74878db6-1908-4dbb-a768-cce1a8bec6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:======================================================>  (25 + 1) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- average_stars: double (nullable = true)\n",
      " |-- compliment_cool: long (nullable = true)\n",
      " |-- compliment_cute: long (nullable = true)\n",
      " |-- compliment_funny: long (nullable = true)\n",
      " |-- compliment_hot: long (nullable = true)\n",
      " |-- compliment_list: long (nullable = true)\n",
      " |-- compliment_more: long (nullable = true)\n",
      " |-- compliment_note: long (nullable = true)\n",
      " |-- compliment_photos: long (nullable = true)\n",
      " |-- compliment_plain: long (nullable = true)\n",
      " |-- compliment_profile: long (nullable = true)\n",
      " |-- compliment_writer: long (nullable = true)\n",
      " |-- cool: long (nullable = true)\n",
      " |-- elite: string (nullable = true)\n",
      " |-- fans: long (nullable = true)\n",
      " |-- friends: string (nullable = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- review_count: long (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- yelping_since: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user = spark.read.json('hdfs:///user/dataproc/yelp_academic_dataset_user.json')\n",
    "user.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c317c307-af49-42b9-b9fc-57cbe3aaddb6",
   "metadata": {},
   "source": [
    "### Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ef2b532-0124-4631-ba2c-bcd8926d6488",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_wide = review.join(business.select('business_id', col('name').alias('biz_name'), 'attributes.RestaurantsTakeOut'),\n",
    "                          on='business_id',\n",
    "                          how='inner'). \\\n",
    "                     join(user.select('user_id', col('name').alias('user_name'), 'fans', 'yelping_since'),\n",
    "                          on='user_id',\n",
    "                          how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23d0eef8-358e-4e12-88b6-944f55425e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:================================================>        (11 + 2) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.6 ms, sys: 3.98 ms, total: 26.5 ms\n",
      "Wall time: 14.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6990247"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "review_wide.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2f7bd6-d73f-426a-a142-be8316b5dd94",
   "metadata": {},
   "source": [
    "- This command take very long!! This is because you are committing also the delayed join!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79b4987f-50b2-49c1-ad66-b4e2cbc3ef5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But we can explictly tell DB to store it in memory first\n",
    "review_wide = review_wide.persist(pyspark.StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5692e980-257c-44c0-8ba6-2c9a5bb1ad59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:=====================================================>(198 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 69.5 ms, sys: 7.25 ms, total: 76.7 ms\n",
      "Wall time: 37.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6990247"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "# Now, this command is still taking even longer! Because this time it runs the Join and keep it in the memory\n",
    "review_wide.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "290ff639-c323-4709-b470-768ef2a077f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:=====================================================>(198 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.17 ms, sys: 45 µs, total: 4.21 ms\n",
      "Wall time: 1.43 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6990247"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "# Now, this command is getting much more faster!\n",
    "review_wide.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe12c96-06bf-4fc7-9a9a-7fc17df11e2c",
   "metadata": {},
   "source": [
    "- And finally!! Counting 6m+ rows in 1.43s!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b7c8ffc-1b1b-466d-8c48-128a5cb788e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+------------------+---------+----+-------------------+\n",
      "|             user_id|         business_id|cool|               date|funny|           review_id|stars|                text|useful|            biz_name|RestaurantsTakeOut|user_name|fans|      yelping_since|\n",
      "+--------------------+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+------------------+---------+----+-------------------+\n",
      "|--RJK834fiQXm21Vp...|aIoUwpy5ZFQXUDxWM...|   0|2019-08-25 23:17:52|    0|QPF7spAqCc-D81GeX...|  1.0|There are new own...|     0|     Pete & Shorty's|              True|    Renee|   0|2018-02-04 20:34:16|\n",
      "|--UhENQdbuWEh0mU5...|K_s-9Wd6vXSfnxYFz...|   1|2017-08-06 02:42:02|    1|dghJt1TSuyFkmLddu...|  5.0|When im first arr...|     0|Kei Sushi Restaurant|              True|    Sonny|   0|2017-06-19 18:37:56|\n",
      "|--cxdcv_b9uhAKsKT...|oD3zBLplcYefdMHo5...|   0|2019-11-01 22:04:32|    0|VNT1ymOYUuWC-qxdI...|  4.0|This is definitel...|     0|  Garden Farm Market|              True|   Cheryl|   0|2014-02-08 01:13:06|\n",
      "|-0EzgKMI9ZakqLiWR...|P0BB_HeVN-M8D31yt...|   0|2019-03-28 21:38:52|    0|ILPE_Jjrqu_DICC7R...|  5.0|Dr. Exelby and Mi...|     0|Rainbow Veterinar...|              null|   Joseph|   0|2018-01-03 21:31:49|\n",
      "|-0EzgKMI9ZakqLiWR...|6aPXOXi8h1m58hihg...|   0|2018-07-18 13:04:59|    0|C74fnh5gpnH2cgbHT...|  5.0|If you're looking...|     0| Old Northeast Pizza|              True|   Joseph|   0|2018-01-03 21:31:49|\n",
      "+--------------------+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+------------------+---------+----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_wide.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f9ce73-af23-49e3-b99f-dc74fc01ceba",
   "metadata": {},
   "source": [
    "### Create View"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
