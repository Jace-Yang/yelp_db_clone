# Need to upgrade pandas to version 0.25 to use "explode" function
# !pip install --upgrade pandas


import pandas as pd
pd.__version__


# Set up your local path
business_json_path = 'business.json'
business = pd.read_json(business_json_path, lines=True)
print(business.shape)
business.head()


# There are significant amount of businesses that are not open anymore
business.is_open.value_counts()


# Keep only business that are still open
# Drop columns that may not be relavent
df_business = business[business['is_open']==1].drop(['hours','is_open','review_count'], axis=1)
print(df_business.shape)
df_business.head()


# df.explode requires pandas ver 0.25
# Create one row for each series that contain comma-separated items
df_explode = df_business.assign(categories = df_business.categories.str.split(', ')).explode('categories')
df_explode.sample(3)


print('Total number of categories:', len(df_explode.categories.value_counts()))
print('Top 10 categories:')
df_explode.categories.value_counts()[:10]


# Finding categories that contains RV
df_explode[df_explode['categories'].str.contains('RV', case=True, na=False)].categories.value_counts()


# Keep only business with categories that are RV related (including Campgrounds)
business_RV = df_business[df_business['categories'].str.contains(
                         'RV Repair|RV Dealers|RV Rental|RV Parks|Campgrounds', case=False, na=False)]
print(business_RV.shape)
business_RV.head()


# Set up your local path
review_json_path = '../data/yelp_dataset/yelp_academic_dataset_review.json'


import pandas as pd

# Set chunk size (smaller if dataset is smaller)
# 2019 Yelp review.json has more than 6 million reviews(rows)
size = 100000
review = pd.read_json(review_json_path, lines=True,
                      # identifying the data type of each column can reduce memory usage
                      dtype={'review_id':str,'user_id':str,'business_id':str,'stars':int,
                             'date':str,'text':str,'useful':int,'funny':int,'cool':int},
                      chunksize=size)


from tqdm import tqdm


# There are multiple chunks to be read
chunk_list = []
for chunk in tqdm(review):
    print(chunk)
    # Drop columns that aren't needed
    chunk = chunk.drop(['review_id','useful','funny','cool'], axis=1)
    # Renaming column name to avoid conflict with business overall star rating
    chunk = chunk.rename(columns={'stars': 'review_stars'})
    # Inner merge with edited business file so only reviews related to the business remain
   # chunk_merged = pd.merge(business_RV, chunk, on='business_id', how='inner')
    # Show feedback on progress
    print(f"{chunk_merged.shape[0]} out of {size:,} related reviews")
    chunk_list.append(chunk_merged)
    
# After trimming down the review file, concatenate all relevant data back to one dataframe
df = pd.concat(chunk_list, ignore_index=True, join='outer', axis=0)
print(df.shape)
df.sample(3)


df.to_csv("yelp_reviews_RV_categories.csv", index=False)
