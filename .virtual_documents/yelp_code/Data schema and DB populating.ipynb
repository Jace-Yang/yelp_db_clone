get_ipython().run_line_magic("load_ext", " sql")


import pandas as pd
import os
from sqlalchemy import *
JACE_DB = True

if JACE_DB:
    DATABASEURI = "postgresql://jy3174:JaceYJH@w4111-4-14.cisxo09blonu.us-east-1.rds.amazonaws.com/proj1part2"
    get_ipython().run_line_magic("sql", " postgresql://jy3174:JaceYJH@w4111-4-14.cisxo09blonu.us-east-1.rds.amazonaws.com/proj1part2")
else:
    DATABASEURI = "postgresql://by2325:0316@w4111-4-14.cisxo09blonu.us-east-1.rds.amazonaws.com/proj1part2"
    get_ipython().run_line_magic("sql", " postgresql://by2325:0316@w4111-4-14.cisxo09blonu.us-east-1.rds.amazonaws.com/proj1part2")
#engine = create_engine(DATABASEURI)


get_ipython().run_cell_magic("sql", "", """
SELECT *
FROM Users
LIMIT 5""")


get_ipython().run_cell_magic("sql", "", """DROP TABLE IF EXISTS Users, Collection_of_User, Review_of_Business, Category, Business, Photo_contained_Business CASCADE;
CREATE TABLE Users(
    user_id text PRIMARY KEY,
    email text UNIQUE NOT NULL,
    name text NOT NULL,
    password text NOT NULL,
    yealping_since date,
    CHECK (length(password) >= 8 AND length(password) <= 16)
);

CREATE TABlE Collection_of_User(
    user_id text,
    collection_id int,
    created_date date,
    PRIMARY KEY(user_id, collection_id),
    FOREIGN KEY(user_id) REFERENCES Users(user_id) ON DELETE CASCADE
);

CREATE TABlE Business(
    business_id text PRIMARY KEY,
    name text,
    address text,
    city text,
    state text,
    postal_code text,
    latitude numeric(4),
    longitude numeric(4),
    is_open boolean,
    is_takeout boolean
);

CREATE TABlE Review_of_Business(   
    review_id int PRIMARY KEY,
    review_date date,
    business_id text NOT NULL,
    -- Attributes of Tip
    short_tip text,
    likes int,
    -- Attributes of Long Review
    detailed_review text,
    stars int,
    useful int,
    funny int,
    cool int,

    CHECK (stars >= 0 AND stars <= 5),
    CHECK (
           ((short_tip IS NULL AND likes IS NULL)  
            OR 
            (detailed_review IS NULL AND stars IS NULL AND useful IS NULL AND funny IS NULL AND cool IS NULL))
           AND
           ((short_tip IS NOT NULL) 
            OR 
            (detailed_review IS NOT NULL))
          ),
    CHECK (length(detailed_review) >= 30 OR detailed_review is NULL),

    FOREIGN KEY(business_id) REFERENCES Business(business_id) ON DELETE CASCADE
);

CREATE TABlE Category(
    name varchar(255) PRIMARY KEY
);

CREATE TABLE Photo_contained_Business(
    photo_id text PRIMARY KEY,
    business_id text NOT NULL,
    caption text,
    label text,
    FOREIGN KEY(business_id) REFERENCES Business
        ON DELETE CASCADE
);""")


get_ipython().run_cell_magic("sql", "", """DROP TABLE IF EXISTS Users_favorite_Business, Users_follow_Collection, Collection_contain_Business,
                     Users_write_Review, Collection_contain_Business,
                     Users_follow_Users, Business_tagged_Category CASCADE;

CREATE TABLE Users_favorite_Business(
    user_id text REFERENCES Users(user_id),
    business_id text REFERENCES Business(business_id),
    PRIMARY KEY(user_id, business_id)
);

CREATE TABLE Users_follow_Collection(
    fan_user_id text REFERENCES Users(user_id),
    followee_user_id text,
    collection_id int,
    PRIMARY KEY(fan_user_id, followee_user_id, collection_id),
    FOREIGN KEY(followee_user_id, collection_id) REFERENCES Collection_of_User(user_id, collection_id)
);

CREATE TABLE Collection_contain_Business(
    collection_owner_id text,
    collection_id int,
    business_id text REFERENCES Business(business_id),
    PRIMARY KEY(collection_owner_id, collection_id, business_id),
    FOREIGN KEY(collection_owner_id, collection_id) REFERENCES Collection_of_User(user_id, collection_id)
);


CREATE TABLE Users_write_Review(
    user_id text NOT NULL REFERENCES Users(user_id) ON DELETE CASCADE, -- allow users to cancel their account
    review_id int REFERENCES Review_of_Business(review_id) ON DELETE CASCADE,
    PRIMARY KEY(review_id)
);

CREATE TABLE Business_tagged_Category(
    business_id text REFERENCES Business,
    name text REFERENCES Category,
    PRIMARY KEY(business_id, name)
);

CREATE TABLE Users_follow_Users(
    follwee_user_id text REFERENCES Users(user_id),
    fan_user_id text REFERENCES Users(user_id),
    follow_since date,
    PRIMARY KEY (follwee_user_id, fan_user_id)
);""")


import pickle
pickle.HIGHEST_PROTOCOL = 4
import pandas as pd
import numpy as np
import random, string
from tqdm import tqdm
from datetime import datetime


N_BUSINESS = 1000
N_USER = 6666


business = pd.read_json("../data/yelp_dataset/yelp_academic_dataset_business.json", lines=True)
photo = pd.read_json('../data/yelp_photos/photos.json', lines=True, dtype={'photo_id':str,'business_id':str,'caption':str,'label':str})


business.state.value_counts()


# Only include restaurant of main category
business_category = business[['business_id', 'categories']]
business_category = business_category.assign(category = business.categories.str.split(', ')).explode('category').drop('categories', axis=1)
MAIN_FOOD_CATEGORIES = """Bars
Sandwiches
Fast Food
Pizza
Coffee & Tea
Breakfast & Brunch
Burgers
Mexican
Specialty Food
Italian
Seafood
Chicken Wings
Chinese
Salad
Bakeries
Cafes""".split('\n')
business_category = business_category[business_category.category.isin(MAIN_FOOD_CATEGORIES)]
business = business.merge(business_category[['business_id']].drop_duplicates(),
                          on="business_id",
                          how="inner").drop(['categories'], axis=1)

# Remove business that doesn't have any photo.
business = business.merge(photo['business_id'].drop_duplicates(), on = "business_id", how = "inner")


# Select N_USER business with moderate amount of reviews
## Note: We finished the data cleaning pipeline, but for the purpose of this homework, we only includes 100 restaurant and 200 users that has wrote reviews on them!
## For project 1 part 3, we will request more resources to allow us populate the whole dataset ^.^
business = business[(business['review_count'] >= business['review_count'].quantile(.25)) & 
                    (business['review_count'] <= business['review_count'].quantile(.75))].sample(n=N_BUSINESS, random_state=4111)
business = business.drop(['review_count', 'stars', 'hours'], axis=1).reset_index(drop=True)

# Extract whether allow takeout information
attribute_values = ['False' if attributes is None else attributes.get('RestaurantsTakeOut') for attributes in business.attributes]
business['is_takeout'] =  ['False' if value is None or value == 'None' else value for value in attribute_values]
business['is_open'] = business['is_open'].astype(bool)
business = business.drop('attributes', axis=1)


business


business.to_pickle('data/Business.pickle')


business_category = business_category.merge(business[['business_id']],
                                            on="business_id",
                                            how="inner")


business_category


business_category.to_pickle('data/Business_tagged_Category.pickle')


photo = pd.merge(photo, business[['business_id']], on=['business_id'], how='inner')


photo


photo.to_pickle("data/Photo_contained_Business.pickle")


category = pd.DataFrame({'name': MAIN_FOOD_CATEGORIES})


category


category.to_pickle('data/Category.pickle')


chunksize = 500000
# Sample N_USER users among those who have commented on those business 
review_json_path = '../data/yelp_dataset/yelp_academic_dataset_review.json'
review = pd.read_json(review_json_path, lines=True, 
                      dtype={'review_id':str,'user_id':str,
                             'business_id':str,'stars':int,
                             'date':str,'text':str,'useful':int,
                             'funny':int,'cool':int},
                      chunksize=chunksize)
chunk_list = []
for chunk in tqdm(review):
    chunk = chunk[['business_id', 'user_id']].merge(business[['business_id']], how='inner')
    chunk_list.append(chunk)
user_id_list = pd.concat(chunk_list, ignore_index=True, axis=0)[["user_id"]].drop_duplicates().sample(n=N_USER)


users_raw = pd.read_pickle('../data/users_raw.pickle')
users = pd.merge(users_raw, user_id_list, how="inner")


# Extract date of registrating
users["yealping_since"] = pd.to_datetime(users["yelping_since"]).dt.date


users["name"].str.replace(' ', '')


'asfdljdaslfjadslkjfasdlf'


# Generate a password for existing users
def password_generator(prefix, length_min=8, length_max=16):
    chars = string.ascii_letters + string.digits + '!@#$%*'
    # Generate a [1, 6], but mostly 1~3 random suffix
    random_suffix_length = round(np.clip(np.random.normal(2, 1), a_min=1, a_max=6))
    random_suffix_length = max(length_min-len(prefix), random_suffix_length)
    random_suffix_length = min(length_max-len(prefix), random_suffix_length)
    random_suffix = ''.join(random.choice(chars) for _ in range(random_suffix_length))
    password = prefix + random_suffix
    if len(password) > 16:
        password = password[:16]
    return password


users['email'] = users['name'].str.replace(' ', '')
users["password"] = users.apply(lambda x: password_generator(prefix=x['email']), axis=1)

# Generate an email address for existing users
EMAIL_DOMAIN_OPTIONS = ["@gmail.com", "@hotmail.com", "@outlook.com", "@inbox.com", "@qq.com"]
users['rank_samename'] = users.groupby("email")["email"].rank(method="first", ascending=True).astype(int)
users['email'] = users.apply(lambda x: x['email'] + "_" +str(x['rank_samename']) + random.choice(EMAIL_DOMAIN_OPTIONS), axis=1).str.replace("_1@", "@", regex=False)

# Select the columns
users = users[["user_id", "email", "name", "password", "yealping_since"]].reset_index(drop=True)


users


users.to_pickle('data/Users.pickle')


collection = users[['user_id', 'yealping_since']].sample(frac=0.15).reset_index(drop=True)

# Generate local id for each user_id
collection['n_collection'] = pd.Series(np.around(np.random.normal(2, 4, len(collection)))).clip(1, 20)
collection['collection_id'] = collection['n_collection'].apply(lambda x: list(range(1, int(x)+1)))
collection = collection.explode('collection_id').reset_index(drop=True)

# Generate a random collection create date
def random_dates(start, end=max(users.yealping_since), n=10):
    random.seed(4111)
    d = random.randint(0, (end - start).days)
    return start + pd.DateOffset(days=d)
collection['created_date'] = collection.apply(lambda x: random_dates(x['yealping_since']), axis=1)


# Select columns
collection = collection[["user_id", "collection_id", "created_date"]]


collection


collection.to_pickle("data/Collection_of_User.pickle")


chunksize = 500000
review_json_path = '../data/yelp_dataset/yelp_academic_dataset_review.json'
review = pd.read_json(review_json_path, lines=True, dtype={'review_id':str,'user_id':str,'business_id':str,'stars':int,'date':str,'text':str,'useful':int,'funny':int,'cool':int},
                      chunksize=chunksize)

chunk_list = []
for chunk in review:
    chunk = chunk.drop(['review_id'], axis=1)
    chunk = chunk.rename(columns={'stars': 'review_stars'})
    chunk = chunk.rename(columns={'text': 'review'})
    chunk_merged = pd.merge(chunk, business[['business_id']], on='business_id', how='inner')
    chunk_merged = pd.merge(chunk_merged, users[['user_id']], on='user_id', how='inner')
    print(f"{chunk_merged.shape[0]} out of {chunksize:,} related reviews")
    chunk_list.append(chunk_merged)

review_df = pd.concat(chunk_list, ignore_index=True, join='outer', axis=0)


chunksize = 500000
tip_json_path = '../data/yelp_dataset/yelp_academic_dataset_tip.json'
tip = pd.read_json(tip_json_path, lines=True, dtype={'text':str,'date':str,'compliment_count':int,'business_id':str,'user_id': int},
                   chunksize=chunksize)

chunk_list_tip = []
for chunk in tip:
    chunk = chunk.rename(columns={'text': 'tip'})
    chunk_merged = pd.merge(chunk, business[['business_id']], on='business_id', how='inner')
    chunk_merged = pd.merge(chunk_merged, users[['user_id']], on='user_id', how='inner')
    print(f"{chunk_merged.shape[0]} out of {chunksize:,} related tips")
    chunk_list_tip.append(chunk_merged)

tip_df = pd.concat(chunk_list_tip, ignore_index=True, join='outer', axis=0)


review_df = review_df[review_df['review'].str.len()>=30]
review_dfs = pd.merge(tip_df, review_df, on=['user_id','business_id','date'], how='outer')
review_dfs = review_dfs.rename(columns={'tip': 'short_tip','date':'review_date','review_stars':'stars','review':'detailed_review','compliment_count':'likes'})
review_dfs['review_date'] = pd.to_datetime(review_dfs['review_date']).dt.date
# review_dfs[['likes','stars','useful','funny','cool','detailed_review']] = review_dfs[['likes','stars','useful','funny','cool', 'detailed_review']]
# review_dfs= review_dfs.where(review_dfs.notnull(),None)


# Add globally unique review_id
review_dfs['review_id'] = [i + 100000 for i in range(len(review_dfs))]

# Select columns
review = review_dfs[['review_id','review_date','business_id','short_tip','likes','detailed_review','stars','useful','funny','cool']]


review


review.to_pickle('data/Review_of_Business.pickle')


users_write_review = review_dfs[['user_id', 'review_id']]


users_write_review


users_write_review.to_pickle('data/Users_write_Review.pickle')


user_business = users[['user_id']].sample(frac=0.4).reset_index(drop=True)

# Generate local id for each user_id
user_business['n_bz_follow'] = pd.Series(np.around(np.random.normal(3, 5, len(user_business)))).clip(lower=1).astype('int')
user_business['business_id'] = user_business['n_bz_follow'].apply(lambda n: business.sample(n).business_id.tolist())
user_business = user_business.explode(["business_id"])
user_business = user_business[['user_id', 'business_id']]


user_business


user_business.to_pickle("data/Users_favorite_Business.pickle")


users_collection = pd.DataFrame()
users_collection['fan_user_id'] = users[['user_id']]

# Random some collections for user to follow
users_collection['n_collection_follow'] = np.round(np.maximum(1, np.random.normal(2, 5, len(users_collection)))).astype('int')
def get_collection(fan):
    df = collection.sample(fan['n_collection_follow'])
    fan['followee_user_id'] = df.user_id.tolist()
    fan['collection_id'] = df.collection_id.tolist()
    return fan
users_collection = users_collection.apply(get_collection, axis=1).explode(["followee_user_id", "collection_id"])
users_collection = users_collection[['fan_user_id','followee_user_id','collection_id']]


collection


users_collection.to_pickle('data/Users_follow_Collection.pickle')


collection_business = pd.DataFrame()
collection_business['collection_owner_id'] = collection['user_id']
collection_business['collection_id'] = collection['collection_id']
collection_business['n_business_contain'] = np.round(np.maximum(1, np.random.normal(3, 5, len(collection_business)))).astype('int')
collection_business['business_id'] = collection_business['n_business_contain'].apply(lambda n: business.sample(n).business_id.tolist())
collection_business = collection_business.explode('business_id')
collection_business = collection_business[['collection_owner_id','collection_id', 'business_id']]


collection_business


collection_business.to_pickle('data/Collection_contain_Business.pickle')


users_follow= pd.DataFrame()
users_follow = users[['user_id']].sample(frac=0.3).reset_index(drop=True) # only 30%users have fans
users_follow = users_follow.rename(columns={'user_id': 'fan_user_id'})

# Generate follower
users_follow['n_followers'] = np.round(np.maximum(1, np.random.normal(3, 5, len(users_follow)))).astype('int')
def get_fan(fan):
    df = users.sample(fan['n_followers'])
    fan['followee_user_id'] = df.user_id.tolist()
    #fan['collection_id'] = df.collection_id.tolist()
    return fan
users_follow = users_follow.apply(get_fan, axis=1).explode(["followee_user_id"])

# Generate follow date
def random_dates(start, end=max(users.yealping_since), n=10):
    random.seed(4111)
    d = random.randint(0, (end - start).days)
    return start + pd.DateOffset(days=d)
users_follow['follow_since'] = users.apply(lambda x: random_dates(x['yealping_since']), axis=1)

users_follow = users_follow[['fan_user_id','followee_user_id','follow_since']]


users_follow


users_follow.to_pickle("data/Users_follow_Users.pickle")


# from google.colab import drive
# drive.mount('/content/drive')
table_names = ['Users',
               'Collection_of_User',
               'Business',
               'Review_of_Business',
               'Category',
               'Photo_contained_Business',
               'Users_favorite_Business',
               'Users_follow_Collection',
               'Users_write_Review', 
               'Collection_contain_Business', 
               'Business_tagged_Category',
               'Users_follow_Users']

folder_path = "data"
file_names = [table_name + '.pickle' for table_name in table_names]
file_paths = [folder_path + '/' + file_name for file_name in file_names]
file_paths


def get_code(table_name):
    df = pd.read_pickle(folder_path + '/' + table_name + '.pickle')
    data_import_code = f'{table_name} = pd.read_pickle(folder_path + "/{table_name}.pickle")'
    #for_loop_code = f'for index, row in {table_name}.iterrows():'
    clean_null_code = f'{table_name} = {table_name}.astype(object).where({table_name}.notna(), None)'
#    table_columns = ", ".join(df.columns)
    value_inser_code = f'engine.execute("""INSERT INTO {table_name} VALUES (' + ', '.join([f'%s' for i in range(len(df.columns))]) + f');""", list({table_name}.itertuples(index=False, name=None)))'
    print('\n'.join([data_import_code, clean_null_code, value_inser_code]))

for table_name in table_names:
    print(f'# Populate {table_name} table')
    get_code(table_name)
    print('')


engine = create_engine(DATABASEURI)


# Populate Users table
Users = pd.read_pickle(folder_path + "/Users.pickle")
Users = Users.astype(object).where(Users.notna(), None)
engine.execute("""INSERT INTO Users VALUES (%s, %s, %s, %s, %s);""", list(Users.itertuples(index=False, name=None)))


# Populate Collection_of_User table
Collection_of_User = pd.read_pickle(folder_path + "/Collection_of_User.pickle")
Collection_of_User = Collection_of_User.astype(object).where(Collection_of_User.notna(), None)
engine.execute("""INSERT INTO Collection_of_User VALUES (%s, %s, %s);""", list(Collection_of_User.itertuples(index=False, name=None)))


# Populate Business table
Business = pd.read_pickle(folder_path + "/Business.pickle")
Business = Business.astype(object).where(Business.notna(), None)
engine.execute("""INSERT INTO Business VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s);""", list(Business.itertuples(index=False, name=None)))


# Populate Review_of_Business table
Review_of_Business = pd.read_pickle(folder_path + "/Review_of_Business.pickle")
Review_of_Business = Review_of_Business.astype(object).where(Review_of_Business.notna(), None)
engine.execute("""INSERT INTO Review_of_Business VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s);""", list(Review_of_Business.itertuples(index=False, name=None)))


# Populate Category table
Category = pd.read_pickle(folder_path + "/Category.pickle")
Category = Category.astype(object).where(Category.notna(), None)
engine.execute("""INSERT INTO Category VALUES (%s);""", list(Category.itertuples(index=False, name=None)))


# Populate Photo_contained_Business table
Photo_contained_Business = pd.read_pickle(folder_path + "/Photo_contained_Business.pickle")
Photo_contained_Business = Photo_contained_Business.astype(object).where(Photo_contained_Business.notna(), None)
engine.execute("""INSERT INTO Photo_contained_Business VALUES (%s, %s, %s, %s);""", list(Photo_contained_Business.itertuples(index=False, name=None)))


# Populate Users_favorite_Business table
Users_favorite_Business = pd.read_pickle(folder_path + "/Users_favorite_Business.pickle")
Users_favorite_Business = Users_favorite_Business.astype(object).where(Users_favorite_Business.notna(), None)
engine.execute("""INSERT INTO Users_favorite_Business VALUES (%s, %s);""", list(Users_favorite_Business.itertuples(index=False, name=None)))


# Populate Users_follow_Collection table
Users_follow_Collection = pd.read_pickle(folder_path + "/Users_follow_Collection.pickle")
Users_follow_Collection = Users_follow_Collection.astype(object).where(Users_follow_Collection.notna(), None)
engine.execute("""INSERT INTO Users_follow_Collection VALUES (%s, %s, %s);""", list(Users_follow_Collection.itertuples(index=False, name=None)))


# Populate Users_write_Review table
Users_write_Review = pd.read_pickle(folder_path + "/Users_write_Review.pickle")
Users_write_Review = Users_write_Review.astype(object).where(Users_write_Review.notna(), None)
engine.execute("""INSERT INTO Users_write_Review VALUES (%s, %s);""", list(Users_write_Review.itertuples(index=False, name=None)))


# Populate Collection_contain_Business table
Collection_contain_Business = pd.read_pickle(folder_path + "/Collection_contain_Business.pickle")
Collection_contain_Business = Collection_contain_Business.astype(object).where(Collection_contain_Business.notna(), None)
engine.execute("""INSERT INTO Collection_contain_Business VALUES (%s, %s, %s);""", list(Collection_contain_Business.itertuples(index=False, name=None)))


# Populate Business_tagged_Category table
Business_tagged_Category = pd.read_pickle(folder_path + "/Business_tagged_Category.pickle")
Business_tagged_Category = Business_tagged_Category.astype(object).where(Business_tagged_Category.notna(), None)
engine.execute("""INSERT INTO Business_tagged_Category VALUES (%s, %s);""", list(Business_tagged_Category.itertuples(index=False, name=None)))


# Populate Users_follow_Users table
Users_follow_Users = pd.read_pickle(folder_path + "/Users_follow_Users.pickle")
Users_follow_Users = Users_follow_Users.astype(object).where(Users_follow_Users.notna(), None)
engine.execute("""INSERT INTO Users_follow_Users VALUES (%s, %s, %s);""", list(Users_follow_Users.itertuples(index=False, name=None)))


Photo_contained_Business = pd.read_pickle(folder_path + "/Photo_contained_Business.pickle")


photo_paths = '../data/yelp_photos/photos/'
photo_names = Photo_contained_Business.photo_id.tolist()


import shutil

front_end_folder = '../yealp_frontend/flaskblog/static/business_photos/'
try:
    shutil.rmtree(front_end_folder)
except:
    pass
os.mkdir(front_end_folder)
for photo_name in photo_names:
    shutil.copy(photo_paths + photo_name + '.jpg' , front_end_folder + photo_name + '.jpg' )


STATES_OPTIONS = [state[0] for state in engine.execute("SELECT state FROM Business GROUP BY state").fetchall()]
STATES_OPTIONS


import json
with open("../yealp_frontend/flaskblog/static/STATES_OPTIONS.json", 'w') as f:
    json.dump(STATES_OPTIONS, f, indent=2) 

# with open("../yealp_frontend/flaskblog/static/STATES_OPTIONS.json", 'r') as f:
#     STATES_OPTIONS = json.load(f)
# STATES_OPTIONS = [(state, state) for state in STATES_OPTIONS]
# STATES_OPTIONS


get_ipython().run_cell_magic("sql", "", """
SELECT name, address, city, round(AVG(stars), 2) AS average_stars
FROM Review_of_Business JOIN Business USING(business_id)
WHERE detailed_review IS NOT NULL AND is_open = True
GROUP BY business_id, name, address, city
HAVING count(*) >= 5
ORDER BY average_stars DESC LIMIT 5""")


get_ipython().run_cell_magic("sql", "", """
WITH user_fans_cnt AS(
    SELECT follwee_user_id as user_id, COUNT(fan_user_id) as fans_cnt
    FROM Users_follow_Users
    GROUP BY follwee_user_id)
SELECT user_id, name, email, yealping_since, fans_cnt
FROM user_fans_cnt JOIN Users USING(user_id)
ORDER BY fans_cnt DESC, yealping_since ASC
LIMIT 5""")


get_ipython().run_cell_magic("sql", "", """
WITH collection_cnt AS(
    SELECT followee_user_id AS collection_owner_id, collection_id, COUNT(fan_user_id) AS fans_cnt
    FROM Users_follow_Collection  
    GROUP BY followee_user_id, collection_id)

SELECT cate.name, 
       COUNT(CONCAT(collection_owner_id, collection_id)) AS n_collections,
       SUM(fans_cnt) as total_fans_among_all_collections
FROM collection_cnt 
    JOIN Collection_contain_Business USING(collection_owner_id, collection_id) 
    JOIN Business USING(business_id)
    JOIN Business_tagged_Category cate USING(business_id)
WHERE is_open = True
GROUP BY cate.name
ORDER BY total_fans_among_all_collections DESC""")


get_ipython().run_cell_magic("sql", "", """
DROP VIEW IF EXISTS business_wide CASCADE;
CREATE VIEW business_wide AS 
    WITH bizs_cal_star AS(
        SELECT business_id, 
               round(AVG(stars), 2) AS average_stars,
               round(round(AVG(stars) * 2) / 2, 1) AS rounded_average_stars
        FROM Review_of_Business JOIN Business USING(business_id)
        WHERE detailed_review IS NOT NULL
        GROUP BY business_id),
    
    bizs_join_category AS(
        SELECT business_id, array_agg(name) as category_names
        FROM Business_tagged_Category
        GROUP BY business_id
    ),
    
    bizs_join_photo AS(
        SELECT business_id, array_agg(photo_id) as photo_ids
        FROM Photo_contained_Business
        GROUP BY business_id
    )
        
    SELECT *
    FROM Business 
        LEFT JOIN bizs_cal_star USING(business_id)
        LEFT JOIN bizs_join_category USING(business_id)
        LEFT JOIN bizs_join_photo USING(business_id)""")


get_ipython().run_cell_magic("sql", "", """
SELECT *
FROM business_wide
LIMIT 5""")


get_ipython().run_cell_magic("sql", "", """
SELECT * FROM
photo_of_business""")


get_ipython().run_cell_magic("sql", "", """
DROP VIEW IF EXISTS reviews_wide CASCADE;
CREATE VIEW reviews_wide AS 

WITH detailed_reviews AS (
    SELECT *
    FROM Review_of_Business
    WHERE detailed_review IS NOT NULL),
    
SELECT review_id, 
       user_id, Users.name as user_name, 
       business_id,
       Business.name as business_name,
       detailed_review, review_date,
       useful, funny, cool	
FROM detailed_reviews
    LEFT JOIN Users_write_Review USING(review_id)
    LEFT JOIN Users USING(user_id)
    LEFT JOIN Business USING(business_id)
ORDER BY review_date DESC""")


get_ipython().run_cell_magic("sql", "", """
SELECT *
FROM reviews_wide
WHERE user_id = 'cPT-E02ZVvzevldYI-vrAg'""")


get_ipython().run_cell_magic("sql", "", """
DROP VIEW IF EXISTS tips_wide CASCADE;
CREATE VIEW tips_wide AS 

WITH tips AS (
    SELECT *
    FROM Review_of_Business
    WHERE short_tip IS NOT NULL)
SELECT review_id, 
       user_id, Users.name as user_name, 
       business_id,
       Business.name as business_name,
       short_tip, review_date, likes
FROM tips
    LEFT JOIN Users_write_Review USING(review_id)
    LEFT JOIN Users USING(user_id)
    LEFT JOIN Business USING(business_id)
ORDER BY review_date DESC""")


get_ipython().run_cell_magic("sql", "", """
SELECT *
FROM business_wide
LIMIT 10""")
