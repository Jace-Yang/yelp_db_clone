get_ipython().run_line_magic("load_ext", " sql")
get_ipython().run_line_magic("sql", " postgresql://by2325:0316@w4111.cisxo09blonu.us-east-1.rds.amazonaws.com/proj1part2")


get_ipython().run_cell_magic("sql", "", """DROP TABLE IF EXISTS Users, Collection_of_User, Review_of_Business, Category, Business, Photo_contained_Business CASCADE;
CREATE TABLE Users(
    user_id text PRIMARY KEY,
    email text UNIQUE NOT NULL,
    name text NOT NULL,
    password text NOT NULL,
    yealping_since date,
    CHECK (length(password) >= 8 AND length(password) <= 16)
);

CREATE TABlE Collection_of_User(
    user_id text,
    collection_id int,
    created_date date,
    PRIMARY KEY(user_id, collection_id),
    FOREIGN KEY(user_id) REFERENCES Users(user_id) ON DELETE CASCADE
);

CREATE TABlE Business(
    business_id text PRIMARY KEY,
    name text,
    address text,
    city text,
    state text,
    postal_code text,
    latitude numeric(4),
    longitude numeric(4),
    is_open boolean,
    is_takeout boolean
);

CREATE TABlE Review_of_Business(   
    review_id int PRIMARY KEY,
    review_date date,
    business_id text NOT NULL,
    -- Attributes of Tip
    short_tip text,
    likes int,
    -- Attributes of Long Review
    detailed_review text,
    stars int,
    useful int,
    funny int,
    cool int,

    CHECK (stars >= 0 AND stars <= 5),
    CHECK (
           ((short_tip IS NULL AND likes IS NULL)  
            OR 
            (detailed_review IS NULL AND stars IS NULL AND useful IS NULL AND funny IS NULL AND cool IS NULL))
           AND
           ((short_tip IS NOT NULL) 
            OR 
            (detailed_review IS NOT NULL))
          ),
    CHECK (length(detailed_review) >= 30 OR detailed_review is NULL),

    FOREIGN KEY(business_id) REFERENCES Business(business_id) ON DELETE CASCADE
);

CREATE TABlE Category(
    name varchar(255) PRIMARY KEY
);

CREATE TABLE Photo_contained_Business(
    photo_id text PRIMARY KEY,
    business_id text NOT NULL,
    caption text,
    label text,
    FOREIGN KEY(business_id) REFERENCES Business
        ON DELETE CASCADE
);""")


get_ipython().run_cell_magic("sql", "", """DROP TABLE IF EXISTS Users_favorite_Business, Users_follow_Collection, Collection_contain_Business,
                     Users_write_Review, Collection_contain_Business,
                     Users_follow_Users, Business_tagged_Category CASCADE;

CREATE TABLE Users_favorite_Business(
    user_id text REFERENCES Users(user_id),
    business_id text REFERENCES Business(business_id),
    PRIMARY KEY(user_id, business_id)
);

CREATE TABLE Users_follow_Collection(
    fan_user_id text REFERENCES Users(user_id),
    followee_user_id text,
    collection_id int,
    PRIMARY KEY(fan_user_id, followee_user_id, collection_id),
    FOREIGN KEY(followee_user_id, collection_id) REFERENCES Collection_of_User(user_id, collection_id)
);

CREATE TABLE Collection_contain_Business(
    collection_owner_id text,
    collection_id int,
    business_id text REFERENCES Business(business_id),
    PRIMARY KEY(collection_owner_id, collection_id, business_id),
    FOREIGN KEY(collection_owner_id, collection_id) REFERENCES Collection_of_User(user_id, collection_id)
);


CREATE TABLE Users_write_Review(
    user_id text NOT NULL REFERENCES Users(user_id) ON DELETE CASCADE, -- allow users to cancel their account
    review_id int REFERENCES Review_of_Business(review_id) ON DELETE CASCADE,
    PRIMARY KEY(review_id)
);

CREATE TABLE Business_tagged_Category(
    business_id text REFERENCES Business,
    name text REFERENCES Category,
    PRIMARY KEY(business_id, name)
);

CREATE TABLE Users_follow_Users(
    follwee_user_id text REFERENCES Users(user_id),
    fan_user_id text REFERENCES Users(user_id),
    follow_since date,
    PRIMARY KEY (follwee_user_id, fan_user_id)
);""")


import pickle
pickle.HIGHEST_PROTOCOL = 4
import pandas as pd
import numpy as np
import random, string
from tqdm import tqdm
from datetime import datetime


business = pd.read_json("../data/yelp_dataset/yelp_academic_dataset_business.json", lines=True)


# Only include restaurant of main category
business_category = business[['business_id', 'categories']]
business_category = business_category.assign(category = business.categories.str.split(', ')).explode('category').drop('categories', axis=1)
MAIN_FOOD_CATEGORIES = """Bars
Sandwiches
Fast Food
Pizza
Coffee & Tea
Breakfast & Brunch
Burgers
Mexican
Specialty Food
Italian
Seafood
Chicken Wings
Chinese
Salad
Bakeries
Cafes""".split('\n')
business_category = business_category[business_category.category.isin(MAIN_FOOD_CATEGORIES)]
business = business.merge(business_category[['business_id']].drop_duplicates(),
                          on="business_id",
                          how="inner").drop(['categories'], axis=1)

# Select N_USER business with moderate amount of reviews
## Note: We finished the data cleaning pipeline, but for the purpose of this homework, we only includes 100 restaurant and 200 users that has wrote reviews on them!
## For project 1 part 3, we will request more resources to allow us populate the whole dataset ^.^
N_BUSINESS = 100
business = business[(business['review_count'] >= business['review_count'].quantile(.25)) & 
                    (business['review_count'] <= business['review_count'].quantile(.75))].sample(n=N_BUSINESS, random_state=4111)
business = business.drop(['review_count', 'stars', 'hours'], axis=1).reset_index(drop=True)

# Extract whether allow takeout information
attribute_values = ['False' if attributes is None else attributes.get('RestaurantsTakeOut') for attributes in business.attributes]
business['is_takeout'] =  ['False' if value is None or value == 'None' else value for value in attribute_values]
business['is_open'] = business['is_open'].astype(bool)
business = business.drop('attributes', axis=1)


business.head(5)


business.to_pickle('data/Business.pickle')


business_category = business_category.merge(business[['business_id']],
                                            on="business_id",
                                            how="inner")


business_category.head(5)


business_category.to_pickle('data/Business_tagged_Category.pickle')


category = pd.DataFrame({'name': MAIN_FOOD_CATEGORIES})


category.head(5)


category.to_pickle('data/Category.pickle')


N_USER = 200 
size = 500000
# Sample N_USER users among those who have commented on those business 
review_json_path = '../data/yelp_dataset/yelp_academic_dataset_review.json'
review = pd.read_json(review_json_path, lines=True, 
                      dtype={'review_id':str,'user_id':str,
                             'business_id':str,'stars':int,
                             'date':str,'text':str,'useful':int,
                             'funny':int,'cool':int},
                      chunksize=size)
chunk_list = []
for chunk in tqdm(review):
    chunk = chunk[['business_id', 'user_id']].merge(business[['business_id']], how='inner')
    chunk_list.append(chunk)
user_id_list = pd.concat(chunk_list, ignore_index=True, axis=0)[["user_id"]].drop_duplicates().sample(n=N_USER)


#users_raw = pd.read_json("../data/yelp_dataset/yelp_academic_dataset_user.json", lines=True)
#users_raw.to_pickle("../data/users_raw.pickle")
users_raw = pd.read_pickle('../data/users_raw.pickle')
users = pd.merge(users_raw, user_id_list, how="inner")


# Extract date of registrating
users["yealping_since"] = pd.to_datetime(users["yelping_since"]).dt.date

# Generate a password for existing users
def password_generator(prefix, length_min=8, length_max=16):
    chars = string.ascii_letters + string.digits + '!@#$%*'
    # Generate a [1, 6], but mostly 1~3 random suffix
    random_suffix_length = round(np.clip(np.random.normal(2, 1), a_min=1, a_max=6))
    random_suffix_length = max(length_min-len(prefix), random_suffix_length)
    random_suffix_length = min(length_max-len(prefix), random_suffix_length)
    random_suffix = ''.join(random.choice(chars) for _ in range(random_suffix_length))
    return prefix+random_suffix

users["password"] = users.apply(lambda x: password_generator(prefix=x['name']), axis=1)

# Generate an email address for existing users
EMAIL_DOMAIN_OPTIONS = ["@gmail.com", "@hotmail.com", "@outlook.com", "@inbox.com", "@qq.com"]
users['email'] = users.groupby("name")["name"].rank(method="first", ascending=True).astype(int)
users['email'] = users.apply(lambda x: x['name'] + "_" +str(x['email']) + random.choice(EMAIL_DOMAIN_OPTIONS), axis=1).str.replace("_1@", "@", regex=False)

# Select the columns
users = users[["user_id", "email", "name", "password", "yealping_since"]].reset_index(drop=True)


users.to_pickle('data/Users.pickle')


collection = users[['user_id', 'yealping_since']].sample(frac=0.15).reset_index(drop=True)

# Generate local id for each user_id
collection['n_collection'] = pd.Series(np.around(np.random.normal(2, 4, len(collection)))).clip(1, 20)
collection['collection_id'] = collection['n_collection'].apply(lambda x: list(range(1, int(x)+1)))
collection = collection.explode('collection_id').reset_index(drop=True)

# Generate a random collection create date
def random_dates(start, end=max(users.yealping_since), n=10):
    random.seed(4111)
    d = random.randint(0, (end - start).days)
    return start + pd.DateOffset(days=d)
collection['created_time'] = collection.apply(lambda x: random_dates(x['yealping_since']), axis=1)

# Select columns
collection = collection[["user_id", "collection_id", "created_time"]]


collection.head()


collection.to_pickle("data/Collection_of_User.pickle")


size = 500000
review_json_path = '../data/yelp_dataset/yelp_academic_dataset_review.json'
review = pd.read_json(review_json_path, lines=True, dtype={'review_id':str,'user_id':str,'business_id':str,'stars':int,'date':str,'text':str,'useful':int,'funny':int,'cool':int},chunksize=size)

chunk_list = []
for chunk in review:
    chunk = chunk.drop(['review_id'], axis=1)
    chunk = chunk.rename(columns={'stars': 'review_stars'})
    chunk = chunk.rename(columns={'text': 'review'})
    chunk_merged = pd.merge(chunk, business[['business_id']], on='business_id', how='inner')
    chunk_merged = pd.merge(chunk_merged, users[['user_id']], on='user_id', how='inner')
    print(f"{chunk_merged.shape[0]} out of {size:,} related reviews")
    chunk_list.append(chunk_merged)

review_df = pd.concat(chunk_list, ignore_index=True, join='outer', axis=0)



size = 500000
tip_json_path = '../data/yelp_dataset/yelp_academic_dataset_tip.json'
tip = pd.read_json(tip_json_path, lines=True, dtype={'text':str,'date':str,'compliment_count':int,'business_id':str,'user_id': int},chunksize=size)

chunk_list_tip = []
for chunk in tip:
    chunk = chunk.rename(columns={'text': 'tip'})
    chunk_merged = pd.merge(chunk, business[['business_id']], on='business_id', how='inner')
    chunk_merged = pd.merge(chunk_merged, users[['user_id']], on='user_id', how='inner')
    print(f"{chunk_merged.shape[0]} out of {size:,} related tips")
    chunk_list_tip.append(chunk_merged)

tip_df = pd.concat(chunk_list_tip, ignore_index=True, join='outer', axis=0)


review_df = review_df[review_df['review'].str.len()>=30]
review_dfs = pd.merge(tip_df, review_df, on=['user_id','business_id','date'], how='outer')
review_dfs = review_dfs.rename(columns={'tip': 'short_tip','date':'review_date','review_stars':'stars','review':'detailed_review','compliment_count':'likes'})
review_dfs['review_date'] =  pd.to_datetime(review_dfs['review_date']).dt.date
# review_dfs[['likes','stars','useful','funny','cool','detailed_review']] = review_dfs[['likes','stars','useful','funny','cool', 'detailed_review']]
#review_dfs= review_dfs.where(review_dfs.notnull(),None)


# Add globally unique review_id
review_dfs['review_id'] = [i + 100000 for i in range(len(review_dfs))]

# Select columns
review = review_dfs[['review_id','review_date','business_id','short_tip','likes','detailed_review','stars','useful','funny','cool']]


review.head()


review.to_pickle('data/Review_of_Business.pickle')


users_write_review = review_dfs[['user_id', 'review_id']]


users_write_review.head()


users_write_review.to_pickle('data/Users_write_Review.pickle')


photo_json_path = '../data/yelp_photos/photos.json'
photo = pd.read_json(photo_json_path, lines=True, dtype={'photo_id':str,'business_id':str,'caption':str,'label':str})                       
photo = pd.merge(photo, business[['business_id']], on=['business_id'], how='inner')


photo.head()


photo.to_pickle("data/Photo_contained_Business.pickle")


user_business = users[['user_id']].sample(frac=0.4).reset_index(drop=True)

# Generate local id for each user_id
user_business['n_bz_follow'] = pd.Series(np.around(np.random.normal(3, 5, len(user_business)))).clip(lower=1).astype('int')
user_business['business_id'] = user_business['n_bz_follow'].apply(lambda n: business.sample(n).business_id.tolist())
user_business = user_business.explode(["business_id"])
user_business = user_business[['user_id', 'business_id']]


user_business.head(5)


user_business.to_pickle("data/Users_favorite_Business.pickle")


users_collection = pd.DataFrame()
users_collection['fan_user_id'] = users[['user_id']]

# Random some collections for user to follow
users_collection['n_collection_follow'] = np.round(np.maximum(1, np.random.normal(2, 5, len(users_collection)))).astype('int')
def get_collection(fan):
    df = collection.sample(fan['n_collection_follow'])
    fan['followee_user_id'] = df.user_id.tolist()
    fan['collection_id'] = df.collection_id.tolist()
    return fan
users_collection = users_collection.apply(get_collection, axis=1).explode(["followee_user_id", "collection_id"])
users_collection = users_collection[['fan_user_id','followee_user_id','collection_id']]


collection.head()


users_collection.to_pickle('data/Users_follow_Collection.pickle')


collection_business = pd.DataFrame()
collection_business['collection_owner_id'] = collection['user_id']
collection_business['collection_id'] = collection['collection_id']
collection_business['n_business_contain'] = np.round(np.maximum(1, np.random.normal(3, 5, len(collection_business)))).astype('int')
collection_business['business_id'] = collection_business['n_business_contain'].apply(lambda n: business.sample(n).business_id.tolist())
collection_business = collection_business.explode('business_id')
collection_business = collection_business[['collection_owner_id','collection_id', 'business_id']]


collection_business.head()


collection_business.to_pickle('data/Collection_contain_Business.pickle')


users_follow= pd.DataFrame()
users_follow = users[['user_id']].sample(frac=0.3).reset_index(drop=True) # only 30%users have fans
users_follow = users_follow.rename(columns={'user_id': 'fan_user_id'})

# Generate follower
users_follow['n_followers'] = np.round(np.maximum(1, np.random.normal(3, 5, len(users_follow)))).astype('int')
def get_fan(fan):
    df = users.sample(fan['n_followers'])
    fan['followee_user_id'] = df.user_id.tolist()
    #fan['collection_id'] = df.collection_id.tolist()
    return fan
users_follow = users_follow.apply(get_fan, axis=1).explode(["followee_user_id"])

# Generate follow date
def random_dates(start, end=max(users.yealping_since), n=10):
    random.seed(4111)
    d = random.randint(0, (end - start).days)
    return start + pd.DateOffset(days=d)
users_follow['follow_since'] = users.apply(lambda x: random_dates(x['yealping_since']), axis=1)

users_follow = users_follow[['fan_user_id','followee_user_id','follow_since']]


users_follow.head()


users_follow.to_pickle("data/Users_follow_Users.pickle")


import pandas as pd
import os
from google.colab import drive
drive.mount('/content/drive')
table_names = ['Users',
               'Collection_of_User',
               'Business',
               'Review_of_Business',
               'Category',
               'Photo_contained_Business',
               'Users_favorite_Business',
               'Users_follow_Collection',
               'Users_write_Review', 
               'Collection_contain_Business', 
               'Business_tagged_Category',
               'Users_follow_Users']

folder_path = "/content/drive/MyDrive/COMS 4111/Project 1/data"
file_names = [table_name + '.pickle' for table_name in table_names]
file_paths = [folder_path + '/' + file_name for file_name in file_names]


def get_code(table_name):
    df = pd.read_pickle(folder_path + '/' + table_name + '.pickle')
    data_import_code = f'{table_name} = pd.read_pickle(folder_path + "/{table_name}.pickle")'
    for_loop_code = f'for index, row in {table_name}.iterrows():'
    clean_null_code = f'{table_name} = {table_name}.astype(object).where({table_name}.notna(), None)'
    value_extract_code = '\n'.join([f'\tvalue{col_idx} = row["{column}"]' for col_idx, column in enumerate(df.columns)])
    value_inser_code = f'\t%sql INSERT INTO {table_name} VALUES (' + ', '.join([f':value{i}' for i in range(len(df.columns))]) + ')'
    print('\n'.join([data_import_code, clean_null_code, for_loop_code, value_extract_code, value_inser_code]))

for table_name in table_names:
    print(f'# Populate {table_name} table')
    get_code(table_name)
    print('')


# Populate Users table
Users = pd.read_pickle(folder_path + "/Users.pickle")
Users = Users.astype(object).where(Users.notna(), None)
for index, row in Users.iterrows():
	value0 = row["user_id"]
	value1 = row["email"]
	value2 = row["name"]
	value3 = row["password"]
	value4 = row["yealping_since"]
	get_ipython().run_line_magic("sql", " INSERT INTO Users VALUES (:value0, :value1, :value2, :value3, :value4)")

# Populate Collection_of_User table
Collection_of_User = pd.read_pickle(folder_path + "/Collection_of_User.pickle")
Collection_of_User = Collection_of_User.astype(object).where(Collection_of_User.notna(), None)
for index, row in Collection_of_User.iterrows():
	value0 = row["user_id"]
	value1 = row["collection_id"]
	value2 = row["created_time"]
	get_ipython().run_line_magic("sql", " INSERT INTO Collection_of_User VALUES (:value0, :value1, :value2)")

# Populate Business table
Business = pd.read_pickle(folder_path + "/Business.pickle")
Business = Business.astype(object).where(Business.notna(), None)
for index, row in Business.iterrows():
	value0 = row["business_id"]
	value1 = row["name"]
	value2 = row["address"]
	value3 = row["city"]
	value4 = row["state"]
	value5 = row["postal_code"]
	value6 = row["latitude"]
	value7 = row["longitude"]
	value8 = row["is_open"]
	value9 = row["is_takeout"]
	get_ipython().run_line_magic("sql", " INSERT INTO Business VALUES (:value0, :value1, :value2, :value3, :value4, :value5, :value6, :value7, :value8, :value9)")

# Populate Review_of_Business table
Review_of_Business = pd.read_pickle(folder_path + "/Review_of_Business.pickle")
Review_of_Business = Review_of_Business.astype(object).where(Review_of_Business.notna(), None)
for index, row in Review_of_Business.iterrows():
	value0 = row["review_id"]
	value1 = row["review_date"]
	value2 = row["business_id"]
	value3 = row["short_tip"]
	value4 = row["likes"]
	value5 = row["detailed_review"]
	value6 = row["stars"]
	value7 = row["useful"]
	value8 = row["funny"]
	value9 = row["cool"]
	get_ipython().run_line_magic("sql", " INSERT INTO Review_of_Business VALUES (:value0, :value1, :value2, :value3, :value4, :value5, :value6, :value7, :value8, :value9)")

# Populate Category table
Category = pd.read_pickle(folder_path + "/Category.pickle")
Category = Category.astype(object).where(Category.notna(), None)
for index, row in Category.iterrows():
	value0 = row["name"]
	get_ipython().run_line_magic("sql", " INSERT INTO Category VALUES (:value0)")

# Populate Photo_contained_Business table
Photo_contained_Business = pd.read_pickle(folder_path + "/Photo_contained_Business.pickle")
Photo_contained_Business = Photo_contained_Business.astype(object).where(Photo_contained_Business.notna(), None)
for index, row in Photo_contained_Business.iterrows():
	value0 = row["photo_id"]
	value1 = row["business_id"]
	value2 = row["caption"]
	value3 = row["label"]
	get_ipython().run_line_magic("sql", " INSERT INTO Photo_contained_Business VALUES (:value0, :value1, :value2, :value3)")

# Populate Users_favorite_Business table
Users_favorite_Business = pd.read_pickle(folder_path + "/Users_favorite_Business.pickle")
Users_favorite_Business = Users_favorite_Business.astype(object).where(Users_favorite_Business.notna(), None)
for index, row in Users_favorite_Business.iterrows():
	value0 = row["user_id"]
	value1 = row["business_id"]
	get_ipython().run_line_magic("sql", " INSERT INTO Users_favorite_Business VALUES (:value0, :value1)")

# Populate Users_follow_Collection table
Users_follow_Collection = pd.read_pickle(folder_path + "/Users_follow_Collection.pickle")
Users_follow_Collection = Users_follow_Collection.astype(object).where(Users_follow_Collection.notna(), None)
for index, row in Users_follow_Collection.iterrows():
	value0 = row["fan_user_id"]
	value1 = row["followee_user_id"]
	value2 = row["collection_id"]
	get_ipython().run_line_magic("sql", " INSERT INTO Users_follow_Collection VALUES (:value0, :value1, :value2)")

# Populate Users_write_Review table
Users_write_Review = pd.read_pickle(folder_path + "/Users_write_Review.pickle")
Users_write_Review = Users_write_Review.astype(object).where(Users_write_Review.notna(), None)
for index, row in Users_write_Review.iterrows():
	value0 = row["user_id"]
	value1 = row["review_id"]
	get_ipython().run_line_magic("sql", " INSERT INTO Users_write_Review VALUES (:value0, :value1)")

# Populate Collection_contain_Business table
Collection_contain_Business = pd.read_pickle(folder_path + "/Collection_contain_Business.pickle")
Collection_contain_Business = Collection_contain_Business.astype(object).where(Collection_contain_Business.notna(), None)
for index, row in Collection_contain_Business.iterrows():
	value0 = row["collection_owner_id"]
	value1 = row["collection_id"]
	value2 = row["business_id"]
	get_ipython().run_line_magic("sql", " INSERT INTO Collection_contain_Business VALUES (:value0, :value1, :value2)")

# Populate Business_tagged_Category table
Business_tagged_Category = pd.read_pickle(folder_path + "/Business_tagged_Category.pickle")
Business_tagged_Category = Business_tagged_Category.astype(object).where(Business_tagged_Category.notna(), None)
for index, row in Business_tagged_Category.iterrows():
	value0 = row["business_id"]
	value1 = row["category"]
	get_ipython().run_line_magic("sql", " INSERT INTO Business_tagged_Category VALUES (:value0, :value1)")

# Populate Users_follow_Users table
Users_follow_Users = pd.read_pickle(folder_path + "/Users_follow_Users.pickle")
Users_follow_Users = Users_follow_Users.astype(object).where(Users_follow_Users.notna(), None)
for index, row in Users_follow_Users.iterrows():
	value0 = row["fan_user_id"]
	value1 = row["followee_user_id"]
	value2 = row["follow_since"]
	get_ipython().run_line_magic("sql", " INSERT INTO Users_follow_Users VALUES (:value0, :value1, :value2)")



get_ipython().run_cell_magic("sql", "", """SELECT name, address, city, round(AVG(stars), 2) AS average_stars
FROM Review_of_Business JOIN Business USING(business_id)
WHERE detailed_review IS NOT NULL AND is_open = True
GROUP BY business_id, name, address, city
HAVING count(*) >= 5
ORDER BY average_stars DESC LIMIT 5""")


get_ipython().run_cell_magic("sql", "", """WITH user_fans_cnt AS(
    SELECT follwee_user_id as user_id, COUNT(fan_user_id) as fans_cnt
    FROM Users_follow_Users
    GROUP BY follwee_user_id)
SELECT user_id, name, email, yealping_since, fans_cnt
FROM user_fans_cnt JOIN Users USING(user_id)
ORDER BY fans_cnt DESC, yealping_since ASC
LIMIT 5""")


get_ipython().run_cell_magic("sql", "", """WITH collection_cnt AS(
    SELECT followee_user_id AS collection_owner_id, collection_id, COUNT(fan_user_id) AS fans_cnt
    FROM Users_follow_Collection  
    GROUP BY followee_user_id, collection_id)

SELECT cate.name, 
       COUNT(CONCAT(collection_owner_id, collection_id)) AS n_collections,
       SUM(fans_cnt) as total_fans_among_all_collections
FROM collection_cnt 
    JOIN Collection_contain_Business USING(collection_owner_id, collection_id) 
    JOIN Business USING(business_id)
    JOIN Business_tagged_Category cate USING(business_id)
WHERE is_open = True
GROUP BY cate.name
ORDER BY total_fans_among_all_collections DESC""")
